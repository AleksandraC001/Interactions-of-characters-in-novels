{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTpdi5W_VY-x"
      },
      "outputs": [],
      "source": [
        "!pip install groq tiktoken\n",
        "!wget https://www.gutenberg.org/files/1342/1342-0.txt -O pride_and_prejudice.txt\n",
        "#download and leave like two chapters for tests\n",
        "\n",
        "import re\n",
        "from groq import Groq\n",
        "from typing import Dict, List, Tuple\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import tiktoken\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "class LLMCharacterTracker:\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = Groq(api_key=api_key)\n",
        "        # Using cl100k_base tokenizer which is compatible with many LLMs\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        self.chunk_size = 500  # tokens per chunk\n",
        "\n",
        "    def preprocess_text(self, input_file: str, output_file: str) -> str:\n",
        "            \"\"\"\n",
        "            Preprocess the book text by removing the introduction, table of contents, ending sections,\n",
        "            text with leading spaces, and text enclosed in square brackets or between underscores.\n",
        "            Removes unwanted punctuation but keeps sentence-ending punctuation marks.\n",
        "            \"\"\"\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # Remove text enclosed in square brackets, including multiline content (e.g., [Illustration: ...])\n",
        "            text = re.sub(r'\\[.*?\\]', '', text, flags=re.DOTALL)\n",
        "\n",
        "            # Remove unwanted punctuation but keep sentence-ending punctuation (., !, ?)\n",
        "            text = re.sub(r\"[\\\"'“”‘’\\]\\[\\(\\){}]\", '', text)\n",
        "\n",
        "            # Print and remove lines with leading spaces\n",
        "            removed_lines = [line for line in text.splitlines() if '      ' in line]\n",
        "            print(\"Lines being removed due to leading spaces:\")\n",
        "            for line in removed_lines:\n",
        "                print(line)\n",
        "\n",
        "            # Remove lines with leading spaces (non-content or formatted text)\n",
        "            text = '\\n'.join([line for line in text.splitlines() if '      ' not in line])\n",
        "\n",
        "            # Identify the start of the main content by looking for the first meaningful paragraph\n",
        "            paragraphs = text.split('\\n\\n')\n",
        "            main_content_index = -1\n",
        "\n",
        "            for i, paragraph in enumerate(paragraphs):\n",
        "                # Check if the paragraph is a valid start (contains more than one word and looks like a complete sentence)\n",
        "                if re.match(r'^[A-Z][^?!.]*[.?!]$', paragraph.strip(), re.MULTILINE) and len(paragraph.split()) > 5:\n",
        "                    main_content_index = i\n",
        "                    break\n",
        "\n",
        "            if main_content_index != -1:\n",
        "                # Retain only the content from the first main paragraph onward\n",
        "                text = '\\n\\n'.join(paragraphs[main_content_index:])\n",
        "\n",
        "            # Identify the ending marker using non-content blocks (e.g., multiple empty lines or formatting markers)\n",
        "            # Ensure these blocks do not include valid content (like \"chapter\")\n",
        "            match = re.search(r'(\\n\\s*\\n\\s*){3,}', text, flags=re.DOTALL)\n",
        "            if match:\n",
        "                surrounding_text = text[max(0, match.start() - 100):match.start() + 100].lower()\n",
        "                if 'chapter' not in surrounding_text:\n",
        "                    text = text[:match.start()] # Remove content after the non-content block\n",
        "\n",
        "            # Remove specific project markers (e.g., \"*** END OF THE PROJECT GUTENBERG\")\n",
        "            text = re.sub(r'\\*\\*\\*.*?\\*\\*\\*', '', text, flags=re.DOTALL)\n",
        "\n",
        "            # Trim empty lines at the start and end of the book\n",
        "            text = text.strip()\n",
        "\n",
        "            # Clean up any extra newlines or spaces for a cleaner output\n",
        "            text = re.sub(r'\\n\\s*\\n', '\\n\\n', text) # Maintain paragraphs with double newlines\n",
        "            text = re.sub(r'-', ' ', text)  # Remove hyphens\n",
        "            text = re.sub(r'[ ]+', ' ', text)  # Normalize spaces\n",
        "            text = re.sub(r'_', '', text) # Remove just the \"_\" symbols while keeping the words intact\n",
        "\n",
        "            with open(output_file, 'w', encoding='utf-8') as out_f:\n",
        "                out_f.write(text)\n",
        "\n",
        "            return output_file\n",
        "\n",
        "    def chunk_text(self, text: str) -> List[Tuple[int, str]]:\n",
        "        \"\"\"\n",
        "        Split text into chunks while preserving paragraph boundaries\n",
        "        \"\"\"\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_tokens = 0\n",
        "        chunk_number = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para_tokens = len(self.tokenizer.encode(para))\n",
        "\n",
        "            # If adding this paragraph would exceed chunk size, finalize current chunk\n",
        "            if current_tokens + para_tokens > self.chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunk_text = '\\n\\n'.join(current_chunk)\n",
        "                    chunks.append((chunk_number, chunk_text))\n",
        "                    current_chunk = []\n",
        "                    current_tokens = 0\n",
        "                    chunk_number += 1\n",
        "\n",
        "            current_chunk.append(para)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "        # Add the final chunk if not empty\n",
        "        if current_chunk:\n",
        "            chunks.append((chunk_number, '\\n\\n'.join(current_chunk)))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def extract_last_sentences(self, text: str, num_sentences: int = 3) -> str:\n",
        "        \"\"\"\n",
        "        Extract the last few sentences from a text while maintaining structural integrity\n",
        "        \"\"\"\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        collected_sentences = []\n",
        "\n",
        "        for paragraph in reversed(paragraphs):\n",
        "            # Split paragraph into sentences\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', paragraph.strip())\n",
        "\n",
        "            # Add sentences from the end of this paragraph\n",
        "            for sentence in reversed(sentences):\n",
        "                if sentence.strip():\n",
        "                    collected_sentences.insert(0, sentence.strip())\n",
        "\n",
        "                    # Stop if we've collected enough sentences\n",
        "                    if len(collected_sentences) >= num_sentences:\n",
        "                        break\n",
        "\n",
        "            # Stop searching if we've collected enough sentences\n",
        "            if len(collected_sentences) >= num_sentences:\n",
        "                break\n",
        "\n",
        "        return ' '.join(collected_sentences[:num_sentences])\n",
        "\n",
        "    def create_prompt(self, chunk: str, previous_context: str = \"\") -> str:\n",
        "        \"\"\"\n",
        "        Create a prompt for the LLM to identify characters and resolve references\n",
        "        \"\"\"\n",
        "        return f\"\"\"Given the following text chunk from a book, identify all character mentions, including pronouns and aliases. Resolve each mention to the character's full name. Consider the previous context if provided.\n",
        "\n",
        "Previous context (if any):\n",
        "{previous_context}\n",
        "\n",
        "Text chunk:\n",
        "{chunk}\n",
        "\n",
        "For each paragraph and sentence in the chunk, list all character mentions in the following format:\n",
        "[Paragraph NUMBER]\n",
        "[Sentence NUMBER]: CharacterMention1[FullName1], CharacterMention2[FullName2], ...\n",
        "\n",
        "Rules:\n",
        "1. Resolve all pronouns (he/she/I/we) to the correct character\n",
        "2. Resolve all aliases and nicknames to the character's full name\n",
        "3. If a mention is already the full name, use the same name for both mention and full name\n",
        "4. Number paragraphs and sentences sequentially\n",
        "5. Include only character mentions, ignore other entities\n",
        "\n",
        "Example output:\n",
        "[Paragraph 1]\n",
        "[Sentence 1]: Lizzy[Elizabeth Bennet], she[Elizabeth Bennet]\n",
        "[Sentence 2]: Mr. Darcy[Fitzwilliam Darcy], they[Fitzwilliam Darcy, Elizabeth Bennet]\n",
        "\n",
        "Please process the text chunk now:\"\"\"\n",
        "\n",
        "    def process_llm_response(self, response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Parse the LLM response into a structured format with proper paragraph numbering\n",
        "        \"\"\"\n",
        "        chronological_mentions = defaultdict(lambda: defaultdict(list))\n",
        "        current_paragraph = 1  # Start with paragraph 1 instead of None\n",
        "\n",
        "        for line in response.strip().split('\\n'):\n",
        "            if line.startswith('[Paragraph'):\n",
        "                # Extract paragraph number, defaulting to incrementing if not found\n",
        "                paragraph_match = re.search(r'\\d+', line)\n",
        "                if paragraph_match:\n",
        "                    current_paragraph = int(paragraph_match.group())\n",
        "            elif line.startswith('[Sentence'):\n",
        "                sentence_match = re.search(r'\\[Sentence (\\d+)\\]:\\s*(.*)', line)\n",
        "                if sentence_match:\n",
        "                    sentence_num = int(sentence_match.group(1))\n",
        "                    mentions_text = sentence_match.group(2)\n",
        "\n",
        "                    # Parse mentions\n",
        "                    mentions = re.findall(r'([^,\\[\\]]+)\\[([^\\[\\]]+)\\]', mentions_text)\n",
        "                    for mention, full_name in mentions:\n",
        "                        mention = mention.strip()\n",
        "                        full_name = full_name.strip()\n",
        "                        chronological_mentions[current_paragraph][sentence_num].append({\n",
        "                            'character': full_name,\n",
        "                            'mention': mention\n",
        "                        })\n",
        "\n",
        "        return chronological_mentions\n",
        "\n",
        "    def merge_mentions(self, all_chunks_mentions: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Merge mentions from multiple chunks while maintaining continuous paragraph numbering\n",
        "        \"\"\"\n",
        "        merged_mentions = defaultdict(lambda: defaultdict(list))\n",
        "        paragraph_offset = 0  # Track the cumulative paragraph count\n",
        "\n",
        "        for chunk_index, chunk_mentions in enumerate(all_chunks_mentions):\n",
        "            # Find the max paragraph number in this chunk\n",
        "            max_para_num = max(int(para_num) for para_num in chunk_mentions.keys()) if chunk_mentions else 0\n",
        "\n",
        "            for para_num, sentences in chunk_mentions.items():\n",
        "                if para_num is not None:\n",
        "                    # Adjust paragraph number based on previous chunks\n",
        "                    adjusted_para_num = int(para_num) + paragraph_offset\n",
        "\n",
        "                    for sent_num, mentions in sentences.items():\n",
        "                        for mention in mentions:\n",
        "                            merged_mentions[adjusted_para_num][sent_num].append(mention)\n",
        "\n",
        "            # Update the offset for the next chunk\n",
        "            paragraph_offset += max_para_num\n",
        "\n",
        "        return merged_mentions\n",
        "\n",
        "    def track_sequential_mentions(self, input_file: str, output_dir: str):\n",
        "        \"\"\"\n",
        "        Process the book using LLM and create chronological tracking of character mentions\n",
        "        \"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Preprocess the text\n",
        "        preprocessed_file = self.preprocess_text(input_file, f\"preprocessed_{input_file}\")\n",
        "\n",
        "        with open(preprocessed_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = self.chunk_text(text)\n",
        "\n",
        "        # Process each chunk\n",
        "        all_chunks_mentions = []\n",
        "        previous_context = \"\"\n",
        "\n",
        "        for chunk_num, chunk_text in chunks:\n",
        "            prompt = self.create_prompt(chunk_text, previous_context)\n",
        "            # Call Groq API\n",
        "            response = self.client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a literary analysis assistant specialized in identifying and tracking characters in narrative text.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ],\n",
        "                model=\"mixtral-8x7b-32768\",\n",
        "                temperature=0.0\n",
        "            )\n",
        "            print(response.choices[0].message.content)\n",
        "            # Process the response\n",
        "            chunk_mentions = self.process_llm_response(response.choices[0].message.content)\n",
        "            all_chunks_mentions.append(chunk_mentions)\n",
        "\n",
        "            # Update context for next chunk\n",
        "            previous_context = self.extract_last_sentences(chunk_text,  num_sentences=5)\n",
        "\n",
        "        # Merge all mentions and write output\n",
        "        merged_mentions = self.merge_mentions(all_chunks_mentions)\n",
        "        self.write_output(merged_mentions, f\"{output_dir}sequential_mentions.txt\")\n",
        "\n",
        "    def write_output(self, chronological_mentions: Dict, output_file: str):\n",
        "        \"\"\"\n",
        "        Write the chronological mentions to the output file\n",
        "        \"\"\"\n",
        "        # Convert defaultdict to regular dict and ensure all keys are integers\n",
        "        mentions_dict = {\n",
        "            int(para_num): {\n",
        "                int(sent_num): mentions\n",
        "                for sent_num, mentions in sentences.items()\n",
        "            }\n",
        "            for para_num, sentences in chronological_mentions.items()\n",
        "            if para_num is not None  # Skip any None keys\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for paragraph in sorted(mentions_dict.keys()):\n",
        "                f.write(f\"\\n[Paragraph {paragraph}]\\n\")\n",
        "                for sentence in sorted(mentions_dict[paragraph].keys()):\n",
        "                    f.write(f\"[Sentence {sentence}]: \")\n",
        "                    mentions = mentions_dict[paragraph][sentence]\n",
        "                    mention_texts = []\n",
        "                    for mention in mentions:\n",
        "                        if mention['mention'] == mention['character']:\n",
        "                            mention_texts.append(f\"{mention['mention']}\")\n",
        "                        else:\n",
        "                            mention_texts.append(f\"{mention['mention']}[{mention['character']}]\")\n",
        "                    f.write(\", \".join(mention_texts))\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tracker = LLMCharacterTracker(api_key=userdata.get(\"GROQ_API_KEY\"))\n",
        "    tracker.track_sequential_mentions(\n",
        "        input_file=\"pride_and_prejudice.txt\",\n",
        "        output_dir=\"pride_and_prejudice_llm/\"\n",
        "    )"
      ]
    }
  ]
}
